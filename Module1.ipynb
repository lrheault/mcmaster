{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Module 1 </h1>\n",
    "<h1> Automated Textual Analysis of Parliamentary Debates </h1>\n",
    "\n",
    "<h3> McMaster Conference on Substantive Representation </h3>\n",
    "\n",
    "<h3> Ludovic Rheault (University of Toronto) </h3>\n",
    "\n",
    "During this module, we will \"learn by doing\".  The plan is to examine concrete examples of computer-assisted textual analysis, and learn the syntax of the Python programming language along the way.  Python is the most popular language in the world (https://spectrum.ieee.org/at-work/innovation/the-2018-top-programming-languages), and also one of the easiest to learn.  \n",
    "\n",
    "This is a gentle introduction to the methods.  For an overview of methods for textual analysis and their applications in political science, you may consult Grimmer and Stewart (2013).\n",
    "\n",
    "<h2> 1. Toy example </h2>\n",
    "\n",
    "Let us start with a simple example to illustrate the concepts.  Suppose we have four \"speeches\" - deliberately simple speeches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "speeches = ['indigenous peoples', 'indigenous affairs',\n",
    "       'international trade', 'trade relations']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1.1. Python List </h3>\n",
    "\n",
    "The Python <b> list </b> is defined with squared brackets, with each elements separated with commas.  \n",
    "\n",
    "In this case, the list contains four items, each containing textual data.  The data type for textual characters is called <b>string</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(speeches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(speeches[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "speeches[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1.2. The Term-Document Matrix </h3>\n",
    "\n",
    "A useful tool for textual analysis is called a term-document matrix (or document-term matrix), which I will abbreviate with TDM.  \n",
    "\n",
    "The objective is to convert the corpus into a numerical matrix representing the count of each word in the vocabulary, for each document. This process is sometimes called vectorization, and will facilitate the analysis of our data using matrix operations.\n",
    "\n",
    "Each row of the matrix represents one document.  Each column represents how many times a word appears in that document.  \n",
    "\n",
    "We will import a library that easily creates a TDM for us.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by initiating an instance of the class CountVectorizer, which converts a list of texts into a term-document matrix.  \n",
    "\n",
    "A <b>class</b> is a fundamental concept in programming.  It means a pre-programmed category of objects that have specific properties, called methods.  For instance, the CountVectorizer class has a method called fit_transform() that converts a corpus into a TDM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The pound symbol indicates that this line is a comment, not executed when running the script. \n",
    "# Here, we passed an option to remove English stop words.  \n",
    "# We create an instance of the CountVectorizer class, called \"tdm\".  It could be any other name.\n",
    "\n",
    "tdm = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use the methods \"fit_transform\" on the list of texts we created, and transform our speeches into a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tdm.fit_transform(speeches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view which column corresponds to which word, we can use the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tdm.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to be clear, here's the matrix we created, printed as a spreadsheet-like dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(X.todense(), columns=tdm.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "speeches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1.3. Fitting a Model </h3>\n",
    "\n",
    "Many models for textual analysis can be fitted from the term-document matrix.  We will start by looking at a simple and elegant topic model, non-negative matrix factorization, to see how it works concretely.\n",
    "\n",
    "We first import the class NMF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we create our own instance of the class, by giving it a name of our choice, so that we can use its methods. \n",
    "\n",
    "As in many topic models, we need to choose the number of topics in advance.  \n",
    "\n",
    "In our toy example, we know that the solution should be two topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nmf = NMF(n_components = 2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now decompose our term-document matrix.  The NMF model decomposes the original TDM into two smaller matrices: \n",
    "\n",
    "$ \\mathbf{W}_{m \\times k}\\mathbf{H}_{k \\times n} \\approx \\mathbf{X}_{m \\times n} $\n",
    "\n",
    "The optimization problem is:\n",
    "\n",
    "$\\min_{\\mathbf{W}, \\mathbf{H}} \\sum_{i,j}(\\mathbf{WH}_{ij} - \\mathbf{X}_{ij})^2$ \n",
    "\n",
    "with the constraints that all elements of $W$ and $H$ be non-negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = nmf.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "H = nmf.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at the W and H matrices.\n",
    "\n",
    "W is a clustering of the documents, by topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(W, speeches, columns=['Topic1', 'Topic2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two documents were assigned to one cluster.  The last two documents to the last cluster.  In other words, the model has learned to detect the two most relevant clusters (or topics) in our TDM.\n",
    "\n",
    "The H matrix gives us the most relevant words for each cluster.  For each row, the largest values indicate anchor words, words that serve to define the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(H, ['Topic1', 'Topic2'], columns=tdm.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word \"indigenous\" best defines the first cluster.  The word \"trade\" best defines the second cluster.  (In this case, they are said to be \"true anchor words\".)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 2. A real-life example: The Canadian Hansard </h2>\n",
    "    \n",
    "Let us use the file hansard_feb_2017.csv.  It contains the speeches made in the month of February 2017 in the Canadian House of Commons, taken from the www.lipad.ca website.\n",
    "\n",
    "The file was saved in the comma-separated values format.  The pandas library (we used it above) allows to load csv files easily into a data frame format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"hansard_feb_2017.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifying the size of our dataset is a useful thing to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing what it looks like (printing the first few rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing all the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the distribution for some of these variables.  \n",
    "\n",
    "The main topics (i.e., the item in the daily order of business) and subtopics (the specific subtitle in the Hansard)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.maintopic.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.subtopic.value_counts().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The party affiliation of the speakers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.speakerparty.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 2.1. Creating a TDM </h3>\n",
    "\n",
    "We can replicate the same steps as above, with our real-life corpus.  We will use the column \"speechtext\" containing the content of the speeches.\n",
    "\n",
    "The TF-IDF (Term-Frequency, Inverse-Document-Frequency) matrix gives more weight to infrequent words, but the idea is similar to our term-document matrix introduced before.  Instead of counts, we are using weighted word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "tdm = TfidfVectorizer(stop_words='english', max_features=2000)\n",
    "X = tdm.fit_transform(df.speechtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 2.2. Fitting a Topic Model </h3>\n",
    "\n",
    "We can now fit a topic model, as we did before.  We'll start with 10 topics to give an example, and look at a metric that can help to choose an optimal number of topics later on.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "nmf = NMF(n_components = 10, random_state = 0)\n",
    "W = nmf.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a function to avoid repeating ourselves.  The function simply says: for each topic, find the words in the H matrix with the highest values, and print them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, top_n):\n",
    "    H = model.components_\n",
    "    for topic_id, topic in enumerate(H):\n",
    "        message = \"Topic #%d: \" % topic_id\n",
    "        message += \" \".join([feature_names[i].replace(' ','_') for i in topic.argsort()[::-1][:top_n]])\n",
    "        print(message)\n",
    "\n",
    "print_top_words(nmf, tdm.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us illustrate Latent Dirichlet Allocation, one of the most popular topic models out there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "tdm = CountVectorizer(stop_words='english', max_features=2000)\n",
    "X = tdm.fit_transform(df.speechtext)\n",
    "lda = LatentDirichletAllocation(n_components = 10, random_state = 0, learning_method = 'online')\n",
    "W = lda.fit_transform(X)\n",
    "\n",
    "print_top_words(lda, tdm.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can improve the model by preprocessing the text.  For an illustration, I have already included a version of the text containing only nouns.  For topic modeling, some parts of speech are more relevant than other.\n",
    "\n",
    "We can also consider n-grams (sequences of more than one word).  \n",
    "\n",
    "These preprocessing steps are sensitive and must be weighed carefully, but understanding them may help to adapt an empirical method to a particular theoretical problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tdm = TfidfVectorizer(ngram_range=(1,2), max_features=2000)\n",
    "X = tdm.fit_transform(df.preprocessed_text)\n",
    "nmf = NMF(n_components = 10, random_state = 0)\n",
    "W = nmf.fit_transform(X)\n",
    "print_top_words(nmf, tdm.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A number of methods to evaluate topic coherence have been proposed in the literature.  For instance, we can implement Mimno et al. (2012)'s topic coherence score.  The closer to zero (the higher the value), the more coherent the topics. \n",
    "\n",
    "A sound analysis involves validating the semantic coherence of the produced topics, in particular by comparing the impact of changing the number of topics, which is the key arbitrary decision that a researcher needs to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def coherence_score(model, tdm, top_n):\n",
    "\n",
    "    W = model.transform(tdm)\n",
    "    H = model.components_\n",
    "    topic_assignnment = np.argmax(W, axis=1)\n",
    "\n",
    "    topic_coherence = []\n",
    "\n",
    "    for topic_id, topic in enumerate(H):\n",
    "\n",
    "        idx = topic_assignnment==topic_id\n",
    "        temp = tdm[idx,:]      \n",
    "        top_words = topic.argsort()[::-1][:top_n]\n",
    "        coherence = 0.0\n",
    "\n",
    "        for i in range(2, len(top_words)):\n",
    "            for j in range(1, i - 1):\n",
    "                \n",
    "                word_i = np.array(temp[:,top_words[i]].todense().tolist())\n",
    "                word_j = np.array(temp[:,top_words[j]].todense().tolist())\n",
    "                               \n",
    "                D12 = np.count_nonzero(word_i * word_j) + 1\n",
    "                D2 = np.count_nonzero(word_j)\n",
    "                \n",
    "                coherence += math.log(D12/D2)\n",
    "                \n",
    "        topic_coherence.append(coherence)\n",
    "\n",
    "    return topic_coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coherence = coherence_score(nmf, X, 10)\n",
    "\n",
    "for topic_id, value in enumerate(coherence):\n",
    "    print(\"Topic %d's Coherence: %0.3f\" %(topic_id, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.mean(coherence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that we are satisfied with our model (we shouldn't be yet, but for the sake of illustration, let's assume we are).\n",
    "\n",
    "We can append the predicted topics to the original data frame.  Next, we can examine the topics produced, using groupings of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['topic'] = np.argmax(W, axis=1)\n",
    "\n",
    "df['carbon_tax'] = np.where(df.topic==1,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.crosstab(df.speakerparty, df.carbon_tax, normalize='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And of course, we can save our enriched dataset for future usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"hansard_feb_2017_v2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 2.4. Using Monroe et al. (2008)'s Fightin' Words algorithm </h3>\n",
    "\n",
    "We can examine the specificity of word usage by party using a technique proposed by political scientists.  Consulting the paper would be required if using in a project, but essentially, the method will compute z-scores indicating which words are more specific to one group of texts compared to another. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fw import FWExtractor\n",
    "\n",
    "liberal = df[df.speakerparty=='Liberal'].speechtext.tolist()\n",
    "cpc = df[df.speakerparty=='Conservative'].speechtext.tolist()\n",
    "\n",
    "tdm = CountVectorizer(stop_words='english', ngram_range=(1,2))\n",
    "f = FWExtractor(cv = tdm)\n",
    "results = f.transform(liberal, cpc)\n",
    "FW = pd.DataFrame(results, columns=['word','freq_liberal','freq_cpc','zscore'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FW.sort_values(by='zscore', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FW.sort_values(by='zscore', ascending=True).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fw import FWExtractor\n",
    "\n",
    "liberal = df[df.speakerparty=='Liberal'].preprocessed_text.tolist()\n",
    "cpc = df[df.speakerparty=='Conservative'].preprocessed_text.tolist()\n",
    "\n",
    "tdm = CountVectorizer(stop_words='english', ngram_range=(1,2))\n",
    "f = FWExtractor(cv = tdm)\n",
    "results = f.transform(liberal, cpc)\n",
    "FW = pd.DataFrame(results, columns=['word','freq_liberal','freq_cpc','zscore'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FW.sort_values(by='zscore', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FW.sort_values(by='zscore', ascending=True).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h2> 3. A More Involved Example </h2>\n",
    "\n",
    "Suppose we have many files, each representing one day of debates.  This is one of the formats available on the Lipad.ca website. We wish to put them all together.  Moreover, we would like to join additional information on MPs, for instance to study representation by gender.\n",
    "\n",
    "Let us look at an example.  The folder 2016/4/ contains daily files with parliamentary speeches (for April 2016).  We can loop over the files and concatenate them into a dataset. \n",
    "\n",
    "Next, the file member_db.csv contains sociodemographic information about Canadian MPs.  We can merge it with the speech dataset to add this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Looping through all daily files in the root directory, and appending the file names:\n",
    "rootdir = '2016/'\n",
    "all_files = []\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        all_files.append(os.path.join(subdir, file))\n",
    "\n",
    "# Concatenating all files at once:         \n",
    "df = pd.concat((pd.read_csv(f) for f in all_files))\n",
    "\n",
    "# Loading the member file:\n",
    "mp_data = pd.read_csv('member_db.csv')\n",
    "\n",
    "# Merging the information in the member file with the speeches, using the MP id key (pid):\n",
    "df = df.merge(mp_data, on='pid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.gender.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 3.1. Sentiment Analysis </h3>\n",
    "\n",
    "We can use the VADER library for Python to compute a sentiment score for each speech (Hutto and Gilbert 2014).  VADER performs valence shifting and accounts for amplifiers/dampeners.  Although conceived for social media, it usually offers high quality sentiment indicators, and can serve as a useful benchmark.\n",
    "\n",
    "We need to download the model first, using the nltk library (a comprehensive library for textual analysis in Python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can import the class, initiate an instance, and compute a compound sentiment score (between -1 and 1) for each speech in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "vader = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A basic example first:\n",
    "example = \"I am not happy.\"\n",
    "vader.polarity_scores(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['sentiment'] = df.speechtext.apply(lambda x: vader.polarity_scores(x)['compound'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate, we can compare the sentiment scores by groupings of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.groupby('gender').sentiment.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can print out the speech with the highest sentiment score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.iloc[df.sentiment.values.argmax()].speechtext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> References </h2>\n",
    "\n",
    "Beelen, Kaspar, Timothy Alberdingk Thijm, Christopher Cochrane, Kees Halvemaan, Graeme Hirst, Michael Kimmins, Sander Lijbrink, Maarten Marx, Nona Naderi, Roman Polyanovsky, Ludovic Rheault, and Tanya Whyte. 2017. \"Digitization of the Canadian Parliamentary Debates.\" Canadian Journal of Political Science 50(3): 849-864.\n",
    "\n",
    "Grimmer, Justin, and Brandon M. Stewart. 2013. \"Text as data: The promise and pitfalls of automatic content analysis methods for political texts.\" Political analysis 21(3): 267-297.\n",
    "\n",
    "Monroe, Burt L., Michael P. Colaresi, and Kevin M. Quinn. 2008. \"Fightin' words: Lexical feature selection and evaluation for identifying the content of political conflict.\" Political Analysis 16(4): 372-403.\n",
    "\n",
    "Hutto, C.J., and Eric Gilbert. 2014. \"Vader: A parsimonious rule-based model for sentiment analysis of social media text.\" Eighth International Conference on Weblogs and Social Media (ICWSM-14).\n",
    "\n",
    "Mimno, David, Hanna M. Wallach, Edmund Talley, Miriam Leenders, and Andrew McCallum. 2011. \"Optimizing Semantic Coherence in Topic Models.\" In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 262–272."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
